{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.8.5 64-bit ('base': conda)"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"6_Training_Inference.ipynb","provenance":[],"collapsed_sections":[]},"interpreter":{"hash":"98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"}},"cells":[{"cell_type":"markdown","source":["# Lesson 6 - Training & Inference\n","- 6강에서는 모델을 학습하고 추론하는 방법에 대해 알아보았습니다.\n","- 이번 실습 자료에서는 다양한 Loss, Optimizer, Scheduler를 활용하는 방법을 알아봅니다.\n","- 또한, Checkpoint, Early Stopping과 같은 학습을 도와주는 Callback 방법을 알아봅니다.\n","- 그리고 Graident Accumulation 방법을 활용하여 학습을 진행해봅니다.\n"],"metadata":{"id":"sound-volume"}},{"cell_type":"markdown","source":["## 1. Loss\n","- Image Classification에 사용되는 다양한 loss 함수들이 존재합니다. 각 loss 함수는 목적이 있고 풀고자 하는 문제에 맞게 적용을 해야합니다.\n","- Cross Entropy Loss는 두 분포간의 불확실성을 최소화 하는 목적을 가진 분류에 사용되는 일반적인 손실함수입니다.\n","- Focal Loss는 Imbalanced Data 문제를 해결하기 위한 손실함수입니다. [참고](https://arxiv.org/pdf/1708.02002.pdf)\n","- Label Smoothing은 학습 데이터의 representation을 더 잘나타내는데 도움을 줍니다. [참고](https://arxiv.org/pdf/1906.02629.pdf)\n","- F1 Loss는 F1 score 향상을 목적으로 하는 손실함수입니다."],"metadata":{"id":"gentle-appeal"}},{"cell_type":"markdown","source":["###### 베이스라인 코드 train.py 파일 중 Loss 관련 부분.\n","- default 로 cross_entropy로 설정되어 있지만 FocalLoss, LabelSmoothingLoss, F1Loss로 바꿔서 적용해 봅시다!"],"metadata":{"id":"90bf3a7f"}},{"cell_type":"code","execution_count":1,"source":["# -- loss & metric\n","criterion = create_criterion(args.criterion)  # default: cross_entropy"],"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'create_criterion' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-361a589a686f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -- loss & metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# default: cross_entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'create_criterion' is not defined"]}],"metadata":{"id":"3fca816b"}},{"cell_type":"markdown","source":["###### 베이스라인 코드 loss.py 파일.\n","- FocalLoss, LabelSmoothingLoss, F1Loss, cross_entropy 중에 원하는 Loss 를 설정하여 학습시킬 수 있습니다. \n","또한 직접 커스터마이징한 Loss를 추가해서 적용해 볼수 있습니다! (아래쪽 Reference를 참고하세요)"],"metadata":{"id":"f8579fef"}},{"cell_type":"code","execution_count":null,"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","# https://discuss.pytorch.org/t/is-this-a-correct-implementation-for-focal-loss-in-pytorch/43327/8\n","class FocalLoss(nn.Module):\n","    def __init__(self, weight=None,\n","                 gamma=2., reduction='mean'):\n","        nn.Module.__init__(self)\n","        self.weight = weight\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, input_tensor, target_tensor):\n","        log_prob = F.log_softmax(input_tensor, dim=-1)\n","        prob = torch.exp(log_prob)\n","        return F.nll_loss(\n","            ((1 - prob) ** self.gamma) * log_prob,\n","            target_tensor,\n","            weight=self.weight,\n","            reduction=self.reduction\n","        )"],"outputs":[],"metadata":{"tags":[],"id":"closed-launch"}},{"cell_type":"code","execution_count":null,"source":["class LabelSmoothingLoss(nn.Module):\n","    def __init__(self, classes=3, smoothing=0.0, dim=-1):\n","        super(LabelSmoothingLoss, self).__init__()\n","        self.confidence = 1.0 - smoothing\n","        self.smoothing = smoothing\n","        self.cls = classes\n","        self.dim = dim\n","\n","    def forward(self, pred, target):\n","        pred = pred.log_softmax(dim=self.dim)\n","        with torch.no_grad():\n","            true_dist = torch.zeros_like(pred)\n","            true_dist.fill_(self.smoothing / (self.cls - 1))\n","            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"],"outputs":[],"metadata":{"tags":[],"id":"filled-configuration"}},{"cell_type":"code","execution_count":null,"source":["# https://gist.github.com/SuperShinyEyes/dcc68a08ff8b615442e3bc6a9b55a354\n","class F1Loss(nn.Module):\n","    def __init__(self, classes=3, epsilon=1e-7):\n","        super().__init__()\n","        self.classes = classes\n","        self.epsilon = epsilon\n","    def forward(self, y_pred, y_true):\n","        assert y_pred.ndim == 2\n","        assert y_true.ndim == 1\n","        y_true = F.one_hot(y_true, self.classes).to(torch.float32)\n","        y_pred = F.softmax(y_pred, dim=1)\n","\n","        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n","        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n","        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","\n","        precision = tp / (tp + fp + self.epsilon)\n","        recall = tp / (tp + fn + self.epsilon)\n","\n","        f1 = 2 * (precision * recall) / (precision + recall + self.epsilon)\n","        f1 = f1.clamp(min=self.epsilon, max=1 - self.epsilon)\n","        return 1 - f1.mean()"],"outputs":[],"metadata":{"tags":[],"id":"informative-banks"}},{"cell_type":"code","execution_count":null,"source":["_criterion_entrypoints = {\n","    'cross_entropy': nn.CrossEntropyLoss,\n","    'focal': FocalLoss,\n","    'label_smoothing': LabelSmoothingLoss,\n","    'f1': F1Loss\n","}"],"outputs":[],"metadata":{"tags":[],"id":"desirable-accounting"}},{"cell_type":"markdown","source":["## 2. Optimizer\n","- 파이토치는 코드를 간단히 수정하여 다양한 optimizer를 사용할 수 있습니다.\n","- 이번 실습 자료에서는 Adam 과 SGD Optimizer 를 활용하는 방법을 알아봅니다.\n","\n","\n","### 2.1 예제를 통해 그래프로 이해하기\n","- 예제를 통해 Optimizer 따라 Loss 값이 변하는 추이를 그래프를 통해서 이해해봅시다.\n","- 비교할 Optimizer 는 베이스라인 코드에서 default 로 사용된 SGD와 Adam Optimizer 입니다."],"metadata":{"id":"gothic-picking"}},{"cell_type":"code","execution_count":2,"source":["import torch\n","import torch.utils.data as Data\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import matplotlib.pyplot as plt\n","\n","import torch.nn as nn\n","\n","# -- 기본 모델 정의.\n","class Net(torch.nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.hidden = torch.nn.Linear(1, 20)   # 은닉층\n","        self.predict = torch.nn.Linear(20, 1)   # 결과층\n","\n","    def forward(self, x):\n","        x = F.relu(self.hidden(x))      # activation function for hidden layer\n","        x = self.predict(x)             # 결과.\n","        return x\n"],"outputs":[],"metadata":{"id":"b9c1313c"}},{"cell_type":"code","execution_count":3,"source":["# -- Hyperparameter 정의.\n","LR = 0.01  #Learning rate\n","BATCH_SIZE = 32 \n","EPOCH = 15 \n","\n","x = torch.unsqueeze(torch.linspace(-1,1,1000),dim=1)\n","y = x.pow(2) + 0.1*torch.normal(torch.zeros(x.size()))\n","\n","# -- 데이터셋 생성.\n","torch_dataset = Data.TensorDataset(x,y)\n","loader = Data.DataLoader(\n","    dataset=torch_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=2\n",")"],"outputs":[],"metadata":{"id":"0f8b3f99"}},{"cell_type":"code","execution_count":4,"source":["# -- 각 optimizer를 사용한 신경망 정의.\n","net_SGD = Net()\n","net_Adam = Net()\n","\n","# -- 각 신경망 리스트.\n","nets = [net_SGD, net_Adam]\n","\n","# -- 각 Optimizer 정의.\n","opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR) # SGD Optimizer\n","opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9,0.99)) # Adam Optimizer\n","\n","# -- 각 Optimizer 리스트.\n","optimizers = [opt_SGD, opt_Adam]\n","\n","# -- Loss 계산.\n","loss_func = torch.nn.MSELoss()\n","# -- Loss 기록.\n","losses_his = [[],[]]"],"outputs":[],"metadata":{"id":"1d863901"}},{"cell_type":"code","execution_count":5,"source":["#-- Training \n","for epoch in range(EPOCH):\n","    print(\"[INFO] Training Epoch : {}\".format(epoch))\n","    for step, (batch_x,batch_y) in enumerate(loader):\n","        \n","        b_x = Variable(batch_x)\n","        b_y = Variable(batch_y)\n","                \n","        for net, opt, l_his in zip(nets, optimizers, losses_his):\n","            output = net(b_x) \n","            loss = loss_func(output, b_y) #각 신경망의 Loss 계산.\n","            \n","            opt.zero_grad() \n","            loss.backward() \n","            opt.step() \n","            \n","\n","            l_his.append(loss.item()) \n","print(\"[INFO] Training ALL DONE!!!\")"],"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Training Epoch : 0\n","[INFO] Training Epoch : 1\n","[INFO] Training Epoch : 2\n","[INFO] Training Epoch : 3\n","[INFO] Training Epoch : 4\n","[INFO] Training Epoch : 5\n","[INFO] Training Epoch : 6\n","[INFO] Training Epoch : 7\n","[INFO] Training Epoch : 8\n","[INFO] Training Epoch : 9\n","[INFO] Training Epoch : 10\n","[INFO] Training Epoch : 11\n","[INFO] Training Epoch : 12\n","[INFO] Training Epoch : 13\n","[INFO] Training Epoch : 14\n","[INFO] Training ALL DONE!!!\n"]}],"metadata":{"id":"8cc5259c"}},{"cell_type":"code","execution_count":6,"source":["#-- 그래프로 시각화.\n","labels = ['SGD', 'Adam']\n","colors = ['darkred', 'royalblue']\n","for i, l_his in enumerate(losses_his):\n","    plt.plot(l_his, label=labels[i], color=colors[i])\n","plt.legend(loc='best')\n","plt.xlabel('Steps')\n","plt.ylabel('Loss')\n","plt.ylim(0,0.25)  \n","plt.show()"],"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"265.995469pt\" version=\"1.1\" viewBox=\"0 0 399.813027 265.995469\" width=\"399.813027pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-08-27T05:36:46.165778</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 265.995469 \nL 399.813027 265.995469 \nL 399.813027 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 228.439219 \nL 384.94375 228.439219 \nL 384.94375 10.999219 \nL 50.14375 10.999219 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mcc47e36c26\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#mcc47e36c26\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(62.180682 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"128.903401\" xlink:href=\"#mcc47e36c26\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(119.359651 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"192.44487\" xlink:href=\"#mcc47e36c26\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(182.90112 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"255.986339\" xlink:href=\"#mcc47e36c26\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(246.442589 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"319.527808\" xlink:href=\"#mcc47e36c26\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(309.984058 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"383.069277\" xlink:href=\"#mcc47e36c26\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(373.525527 243.037656)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Steps -->\n     <g transform=\"translate(203.553906 256.715781)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-83\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"102.685547\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"164.208984\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"227.685547\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m477e84d47d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m477e84d47d\" y=\"228.439219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.00 -->\n      <g transform=\"translate(20.878125 232.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m477e84d47d\" y=\"184.951219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.05 -->\n      <g transform=\"translate(20.878125 188.750437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m477e84d47d\" y=\"141.463219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.10 -->\n      <g transform=\"translate(20.878125 145.262437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m477e84d47d\" y=\"97.975219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.15 -->\n      <g transform=\"translate(20.878125 101.774437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m477e84d47d\" y=\"54.487219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.20 -->\n      <g transform=\"translate(20.878125 58.286437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m477e84d47d\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.25 -->\n      <g transform=\"translate(20.878125 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- Loss -->\n     <g transform=\"translate(14.798438 130.686406)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"115.144531\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"167.244141\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#pd98a694769)\" d=\"M 65.361932 66.964744 \nL 66.632761 140.257865 \nL 67.268176 130.247446 \nL 67.903591 161.229622 \nL 68.539005 159.293139 \nL 69.17442 173.174496 \nL 69.809835 111.037607 \nL 71.080664 167.413045 \nL 72.351493 121.87382 \nL 72.986908 136.957069 \nL 73.622323 144.911159 \nL 74.257737 147.620765 \nL 74.893152 134.277175 \nL 75.528567 159.456706 \nL 76.163982 150.726927 \nL 76.799396 109.775118 \nL 77.434811 136.528753 \nL 78.070226 154.289608 \nL 78.70564 155.24568 \nL 79.341055 138.813826 \nL 79.97647 158.315242 \nL 80.611884 99.268652 \nL 81.247299 159.060111 \nL 81.882714 118.795749 \nL 82.518128 161.097024 \nL 83.153543 119.151435 \nL 83.788958 155.693126 \nL 84.424373 172.153998 \nL 85.059787 162.804302 \nL 85.695202 151.184204 \nL 86.330617 125.413671 \nL 86.966031 84.826361 \nL 87.601446 162.218782 \nL 88.236861 148.593102 \nL 88.872275 122.543369 \nL 89.50769 160.412428 \nL 90.143105 151.539955 \nL 91.413934 177.064224 \nL 92.049349 155.345566 \nL 92.684763 166.212209 \nL 93.320178 127.836896 \nL 93.955593 138.591049 \nL 94.591008 169.084527 \nL 95.861837 137.149778 \nL 96.497252 116.144303 \nL 97.132666 149.527005 \nL 97.768081 140.52623 \nL 98.403496 137.476575 \nL 99.03891 161.065764 \nL 99.674325 178.447134 \nL 100.30974 146.923092 \nL 100.945154 148.71353 \nL 101.580569 147.569299 \nL 102.215984 160.2139 \nL 102.851399 139.033006 \nL 103.486813 141.70804 \nL 104.122228 176.66825 \nL 104.757643 88.31369 \nL 105.393057 149.131213 \nL 106.028472 159.995802 \nL 106.663887 163.644624 \nL 107.299301 164.456369 \nL 107.934716 120.505185 \nL 108.570131 160.840472 \nL 109.205545 163.131002 \nL 109.84096 160.046568 \nL 110.476375 153.541376 \nL 111.111789 151.41385 \nL 111.747204 147.731868 \nL 112.382619 155.211302 \nL 113.018034 143.064155 \nL 113.653448 162.702038 \nL 114.288863 140.762376 \nL 114.924278 166.556231 \nL 115.559692 133.376166 \nL 116.195107 126.802789 \nL 116.830522 151.841602 \nL 117.465936 153.838514 \nL 118.101351 128.822407 \nL 118.736766 121.941849 \nL 119.37218 154.530789 \nL 120.007595 164.598959 \nL 120.64301 138.285397 \nL 121.278425 158.031726 \nL 121.913839 146.211486 \nL 122.549254 156.412035 \nL 123.184669 171.700085 \nL 123.820083 144.173218 \nL 124.455498 144.915482 \nL 125.090913 158.3334 \nL 125.726327 152.711345 \nL 126.361742 190.513742 \nL 126.997157 162.368546 \nL 128.267986 149.822535 \nL 128.903401 159.77574 \nL 129.538815 136.052923 \nL 130.17423 161.254033 \nL 130.809645 138.838205 \nL 131.44506 151.6971 \nL 132.080474 157.304166 \nL 132.715889 169.261074 \nL 133.351304 133.95921 \nL 133.986718 144.103413 \nL 134.622133 143.908002 \nL 135.257548 137.950046 \nL 135.892962 149.284152 \nL 136.528377 141.35273 \nL 137.163792 140.837624 \nL 137.799206 145.979793 \nL 138.434621 165.177384 \nL 139.070036 137.137018 \nL 139.705451 168.970268 \nL 140.340865 137.159252 \nL 140.97628 167.937839 \nL 141.611695 163.101582 \nL 142.247109 177.476612 \nL 142.882524 159.424701 \nL 143.517939 185.111701 \nL 144.153353 159.225816 \nL 144.788768 143.035033 \nL 145.424183 147.23632 \nL 146.059597 176.179457 \nL 146.695012 163.332433 \nL 147.330427 186.785139 \nL 147.965841 161.661056 \nL 148.601256 151.498384 \nL 149.236671 127.615486 \nL 149.872086 163.087922 \nL 150.5075 165.451685 \nL 151.142915 163.658447 \nL 151.77833 152.43924 \nL 152.413744 161.687158 \nL 153.049159 155.398723 \nL 153.684574 165.613236 \nL 154.319988 159.764497 \nL 154.955403 177.767927 \nL 155.590818 156.751935 \nL 156.226232 163.798562 \nL 156.861647 146.323847 \nL 157.497062 151.740401 \nL 158.132477 148.522805 \nL 158.767891 153.354889 \nL 159.403306 180.308972 \nL 160.038721 138.843687 \nL 160.674135 150.69916 \nL 161.30955 127.737923 \nL 162.580379 175.473295 \nL 163.215794 145.050413 \nL 163.851209 169.324061 \nL 164.486623 174.312763 \nL 165.122038 169.71452 \nL 165.757453 140.24344 \nL 166.392867 157.080314 \nL 167.028282 150.315485 \nL 167.663697 164.20307 \nL 168.299112 131.590626 \nL 168.934526 150.313114 \nL 169.569941 182.133354 \nL 170.205356 161.073054 \nL 170.84077 148.626385 \nL 171.476185 181.98508 \nL 172.1116 155.256865 \nL 172.747014 165.642929 \nL 173.382429 186.577393 \nL 174.017844 155.197059 \nL 174.653258 184.175753 \nL 175.288673 136.417954 \nL 175.924088 169.205843 \nL 176.559503 148.189132 \nL 177.194917 151.387922 \nL 177.830332 182.339947 \nL 178.465747 166.519864 \nL 179.101161 185.274208 \nL 179.736576 185.9688 \nL 180.371991 154.045395 \nL 181.007405 180.091282 \nL 181.64282 139.168527 \nL 182.278235 149.110502 \nL 182.913649 162.593423 \nL 183.549064 146.781227 \nL 184.184479 177.172307 \nL 184.819893 151.66142 \nL 185.455308 165.986465 \nL 186.090723 135.150793 \nL 186.726138 132.710175 \nL 187.361552 166.578063 \nL 187.996967 154.873003 \nL 188.632382 157.353027 \nL 189.267796 162.653721 \nL 189.903211 158.911234 \nL 190.538626 164.306144 \nL 191.17404 163.360603 \nL 191.809455 179.456393 \nL 192.44487 172.206864 \nL 193.080284 158.613494 \nL 193.715699 133.150771 \nL 194.351114 148.123222 \nL 194.986529 174.658634 \nL 195.621943 181.267766 \nL 196.257358 166.712152 \nL 196.892773 173.509971 \nL 197.528187 179.084759 \nL 198.163602 170.627285 \nL 198.799017 158.519803 \nL 199.434431 168.342639 \nL 200.069846 173.504132 \nL 200.705261 167.965594 \nL 201.340675 151.754411 \nL 201.97609 150.886133 \nL 202.611505 159.803865 \nL 203.246919 164.815839 \nL 203.882334 187.084308 \nL 204.517749 168.176434 \nL 205.153164 156.991956 \nL 205.788578 167.609584 \nL 206.423993 169.307978 \nL 207.059408 122.398912 \nL 207.694822 164.401987 \nL 208.330237 171.377156 \nL 208.965652 172.599669 \nL 209.601066 160.347121 \nL 210.236481 142.916484 \nL 210.871896 147.433694 \nL 211.50731 165.576066 \nL 212.142725 166.727912 \nL 212.77814 172.185025 \nL 214.048969 175.922844 \nL 214.684384 159.886351 \nL 215.319799 185.71055 \nL 215.955213 152.058482 \nL 216.590628 155.573378 \nL 217.226043 165.208236 \nL 217.861457 171.450765 \nL 218.496872 137.545674 \nL 219.132287 193.495193 \nL 219.767701 187.073991 \nL 220.403116 167.51798 \nL 221.038531 176.068943 \nL 221.673945 177.434192 \nL 222.30936 174.022037 \nL 222.944775 154.627208 \nL 223.58019 152.626836 \nL 224.215604 173.093869 \nL 224.851019 177.784075 \nL 225.486434 165.428434 \nL 226.121848 164.316461 \nL 226.757263 185.964536 \nL 227.392678 173.101425 \nL 228.028092 150.897253 \nL 228.663507 171.095208 \nL 229.298922 166.613685 \nL 229.934336 182.771665 \nL 230.569751 175.919591 \nL 231.205166 165.263882 \nL 231.840581 145.360964 \nL 232.475995 166.214354 \nL 233.11141 153.385294 \nL 233.746825 192.260819 \nL 234.382239 174.277688 \nL 235.017654 181.261502 \nL 235.653069 174.274571 \nL 236.288483 172.230264 \nL 236.923898 145.628539 \nL 237.559313 170.343814 \nL 238.194727 187.583599 \nL 238.830142 184.511678 \nL 239.465557 158.784701 \nL 240.100971 168.871749 \nL 240.736386 153.404281 \nL 241.371801 177.282189 \nL 242.007216 159.650575 \nL 242.64263 171.628174 \nL 243.278045 177.455943 \nL 243.91346 174.691645 \nL 244.548874 173.17808 \nL 245.184289 178.274142 \nL 245.819704 172.395684 \nL 246.455118 169.387082 \nL 247.090533 184.282388 \nL 247.725948 195.087181 \nL 248.361362 178.339916 \nL 248.996777 144.110244 \nL 249.632192 187.414167 \nL 250.267607 194.321625 \nL 250.903021 191.694098 \nL 252.173851 158.617006 \nL 252.809265 154.567564 \nL 253.44468 164.57773 \nL 254.080095 182.765545 \nL 254.715509 171.065173 \nL 255.350924 166.597886 \nL 255.986339 165.025986 \nL 256.621753 185.965738 \nL 257.257168 153.530788 \nL 257.892583 165.365958 \nL 258.527997 179.891238 \nL 259.163412 160.03909 \nL 259.798827 193.98988 \nL 260.434242 190.064994 \nL 261.069656 189.754008 \nL 261.705071 159.893052 \nL 262.340486 190.239023 \nL 262.9759 172.699723 \nL 263.611315 166.142975 \nL 264.24673 164.372307 \nL 264.882144 168.94644 \nL 265.517559 175.777839 \nL 266.152974 161.307761 \nL 266.788388 182.814846 \nL 267.423803 174.227541 \nL 268.059218 157.462199 \nL 268.694633 164.006279 \nL 269.330047 173.197274 \nL 269.965462 193.0341 \nL 270.600877 180.678438 \nL 271.236291 198.752395 \nL 271.871706 181.662748 \nL 272.507121 192.401912 \nL 273.142535 175.244563 \nL 273.77795 171.186735 \nL 274.413365 186.906154 \nL 275.048779 163.94042 \nL 275.684194 173.045533 \nL 276.955023 197.785667 \nL 277.590438 170.854248 \nL 278.225853 187.190097 \nL 278.861268 159.32449 \nL 279.496682 143.07407 \nL 280.132097 175.012409 \nL 280.767512 172.032306 \nL 281.402926 168.429202 \nL 282.038341 177.518121 \nL 282.673756 165.971612 \nL 283.30917 174.477862 \nL 283.944585 185.513847 \nL 284.58 166.160484 \nL 285.215414 186.738692 \nL 285.850829 165.903349 \nL 286.486244 179.974907 \nL 287.121659 174.750741 \nL 287.757073 165.600056 \nL 288.392488 161.152313 \nL 289.027903 154.124836 \nL 289.663317 179.84715 \nL 290.934147 183.982844 \nL 291.569561 169.319363 \nL 292.204976 176.758335 \nL 292.840391 174.735901 \nL 293.475805 167.553776 \nL 294.11122 146.26824 \nL 294.746635 178.30791 \nL 295.382049 182.052659 \nL 296.017464 171.691032 \nL 296.652879 171.059088 \nL 297.288294 194.267366 \nL 297.923708 176.03895 \nL 298.559123 177.114083 \nL 299.194538 157.13419 \nL 299.829952 177.06678 \nL 300.465367 190.623057 \nL 301.100782 186.337392 \nL 301.736196 185.516905 \nL 302.371611 192.581661 \nL 303.007026 197.09648 \nL 303.64244 187.254109 \nL 304.277855 170.124718 \nL 304.91327 193.946219 \nL 305.548685 181.804447 \nL 306.184099 178.742438 \nL 306.819514 179.296655 \nL 307.454929 172.044353 \nL 308.090343 192.922915 \nL 308.725758 204.560264 \nL 309.361173 179.676273 \nL 309.996587 189.871627 \nL 310.632002 180.460813 \nL 311.267417 190.761015 \nL 311.902831 181.295381 \nL 313.173661 174.300544 \nL 313.809075 183.767464 \nL 314.44449 180.749086 \nL 315.079905 186.374815 \nL 315.71532 189.792144 \nL 316.350734 151.420648 \nL 316.986149 189.35993 \nL 317.621564 180.215628 \nL 318.256978 179.283831 \nL 318.892393 176.211136 \nL 319.527808 178.270163 \nL 320.163222 177.40079 \nL 320.798637 177.446826 \nL 321.434052 178.908688 \nL 322.069466 191.185699 \nL 322.704881 186.764269 \nL 323.340296 187.776447 \nL 323.975711 175.92134 \nL 324.611125 172.361877 \nL 325.24654 172.806336 \nL 325.881955 174.850465 \nL 326.517369 162.510748 \nL 327.152784 192.85034 \nL 327.788199 198.75141 \nL 328.423613 190.006581 \nL 329.059028 178.964803 \nL 329.694443 186.204832 \nL 330.329857 184.981296 \nL 330.965272 189.997535 \nL 331.600687 160.000338 \nL 332.236101 196.080647 \nL 332.871516 191.920546 \nL 333.506931 197.671369 \nL 334.142346 199.166864 \nL 334.77776 181.62489 \nL 335.413175 183.899981 \nL 336.04859 187.854689 \nL 336.684004 167.457487 \nL 337.954834 196.261818 \nL 338.590248 174.15425 \nL 339.225663 172.794652 \nL 339.861078 164.18939 \nL 340.496492 190.816433 \nL 341.131907 179.832871 \nL 341.767322 187.954556 \nL 342.402737 190.248484 \nL 343.038151 162.684003 \nL 343.673566 173.251086 \nL 344.308981 188.052232 \nL 344.944395 190.065214 \nL 345.57981 150.509555 \nL 346.215225 194.043031 \nL 346.850639 196.517505 \nL 347.486054 196.661025 \nL 348.121469 159.634945 \nL 348.756883 198.143109 \nL 349.392298 211.043885 \nL 350.027713 186.364447 \nL 350.663127 190.586172 \nL 351.298542 191.490752 \nL 351.933957 175.188097 \nL 352.569372 191.984317 \nL 353.204786 182.71149 \nL 353.840201 191.948194 \nL 354.475616 191.944147 \nL 355.11103 170.502502 \nL 355.746445 183.143066 \nL 356.38186 192.109839 \nL 357.017274 179.157765 \nL 357.652689 185.773865 \nL 358.288104 201.542657 \nL 358.923518 175.959045 \nL 359.558933 185.204306 \nL 360.194348 202.395813 \nL 360.829763 191.475957 \nL 361.465177 175.001733 \nL 362.100592 162.060723 \nL 362.736007 192.054787 \nL 363.371421 164.063778 \nL 364.006836 195.66419 \nL 364.642251 180.88207 \nL 365.277665 195.877042 \nL 365.91308 191.772304 \nL 366.548495 169.222653 \nL 367.183909 186.023496 \nL 367.819324 175.629591 \nL 368.454739 195.910386 \nL 369.090153 193.611934 \nL 369.725568 204.31589 \nL 369.725568 204.31589 \n\" style=\"fill:none;stroke:#8b0000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#pd98a694769)\" d=\"M 65.361932 6.627327 \nL 65.997347 79.801601 \nL 66.632761 135.092704 \nL 67.268176 130.952545 \nL 67.903591 156.341873 \nL 68.539005 147.792342 \nL 69.17442 168.016386 \nL 69.809835 126.822061 \nL 70.445249 146.829524 \nL 71.080664 175.979173 \nL 71.716079 165.805718 \nL 72.351493 146.972471 \nL 73.622323 168.726981 \nL 74.257737 162.662884 \nL 74.893152 157.863428 \nL 75.528567 183.930283 \nL 76.163982 168.613869 \nL 76.799396 156.43178 \nL 77.434811 166.400602 \nL 78.070226 180.051024 \nL 78.70564 181.801842 \nL 79.341055 179.360586 \nL 79.97647 195.639717 \nL 80.611884 159.191056 \nL 81.247299 198.415907 \nL 81.882714 188.20777 \nL 82.518128 187.558605 \nL 83.153543 182.843693 \nL 83.788958 204.72869 \nL 84.424373 200.2217 \nL 85.059787 206.196297 \nL 85.695202 205.019909 \nL 86.330617 196.30822 \nL 86.966031 181.591443 \nL 87.601446 209.285259 \nL 88.236861 210.781578 \nL 88.872275 193.996192 \nL 89.50769 209.740927 \nL 90.143105 204.043645 \nL 90.778519 214.640885 \nL 91.413934 219.486191 \nL 92.049349 209.254123 \nL 92.684763 212.827551 \nL 93.320178 209.44686 \nL 93.955593 208.260722 \nL 94.591008 213.678488 \nL 95.226422 215.518726 \nL 95.861837 208.142873 \nL 96.497252 213.145573 \nL 97.132666 212.414705 \nL 98.403496 215.838012 \nL 99.03891 205.682048 \nL 99.674325 215.211435 \nL 100.30974 209.039921 \nL 100.945154 219.980012 \nL 101.580569 212.722277 \nL 102.215984 216.236492 \nL 102.851399 214.975137 \nL 103.486813 214.221725 \nL 104.122228 213.901954 \nL 104.757643 206.527101 \nL 105.393057 219.428544 \nL 106.028472 215.704856 \nL 106.663887 217.686446 \nL 107.299301 216.544672 \nL 107.934716 213.982942 \nL 108.570131 216.10944 \nL 109.205545 221.01939 \nL 109.84096 216.449405 \nL 110.476375 218.036422 \nL 111.111789 219.030465 \nL 111.747204 219.223817 \nL 112.382619 217.381466 \nL 113.018034 217.286638 \nL 113.653448 217.045883 \nL 114.288863 218.051718 \nL 114.924278 216.758303 \nL 115.559692 220.85443 \nL 116.195107 214.396676 \nL 116.830522 215.645731 \nL 117.465936 210.439455 \nL 118.101351 204.312878 \nL 119.37218 217.828217 \nL 120.007595 219.336194 \nL 120.64301 218.414705 \nL 121.278425 214.468639 \nL 121.913839 215.515278 \nL 122.549254 218.601779 \nL 123.184669 215.283793 \nL 123.820083 221.221763 \nL 124.455498 217.34047 \nL 125.090913 214.251808 \nL 125.726327 208.793631 \nL 126.361742 220.12674 \nL 126.997157 217.031506 \nL 127.632571 218.984457 \nL 128.267986 216.704354 \nL 128.903401 219.894254 \nL 129.538815 217.176267 \nL 130.17423 216.466422 \nL 130.809645 211.678392 \nL 131.44506 217.634797 \nL 132.080474 218.478301 \nL 132.715889 221.464023 \nL 133.351304 214.810252 \nL 133.986718 220.983585 \nL 134.622133 216.102689 \nL 135.257548 215.392977 \nL 135.892962 215.955265 \nL 136.528377 214.897011 \nL 137.163792 220.94006 \nL 137.799206 213.972899 \nL 138.434621 218.274938 \nL 139.070036 216.730126 \nL 139.705451 220.61817 \nL 140.340865 216.633773 \nL 140.97628 218.246704 \nL 141.611695 217.447861 \nL 142.247109 213.962932 \nL 142.882524 217.620789 \nL 143.517939 217.853014 \nL 144.153353 220.068245 \nL 144.788768 212.673149 \nL 145.424183 216.563202 \nL 146.059597 218.096027 \nL 146.695012 221.918323 \nL 147.330427 218.31592 \nL 147.965841 211.221652 \nL 148.601256 219.009593 \nL 149.236671 207.419216 \nL 149.872086 217.443505 \nL 150.5075 217.828306 \nL 151.142915 218.731701 \nL 151.77833 221.704759 \nL 152.413744 220.077194 \nL 153.049159 219.087068 \nL 153.684574 209.229602 \nL 154.319988 220.187058 \nL 154.955403 220.621057 \nL 155.590818 214.130156 \nL 156.226232 222.394591 \nL 156.861647 217.875582 \nL 157.497062 216.658107 \nL 158.132477 212.219952 \nL 158.767891 220.101624 \nL 159.403306 220.390486 \nL 160.038721 216.846108 \nL 160.674135 216.819791 \nL 161.30955 210.895651 \nL 162.580379 218.894886 \nL 163.215794 219.135255 \nL 163.851209 221.592124 \nL 164.486623 220.881041 \nL 165.122038 218.382671 \nL 165.757453 222.676627 \nL 166.392867 205.653609 \nL 167.028282 218.576634 \nL 167.663697 223.21408 \nL 168.299112 218.320498 \nL 168.934526 217.720177 \nL 169.569941 220.804073 \nL 170.205356 220.412693 \nL 170.84077 217.654903 \nL 171.476185 218.136521 \nL 172.1116 220.046415 \nL 172.747014 216.513438 \nL 173.382429 223.050662 \nL 174.017844 218.077039 \nL 174.653258 220.272001 \nL 175.288673 218.335783 \nL 175.924088 221.28249 \nL 176.559503 215.921063 \nL 177.194917 212.998927 \nL 177.830332 216.440653 \nL 178.465747 216.114522 \nL 179.101161 213.465945 \nL 179.736576 216.663146 \nL 180.371991 218.808361 \nL 181.007405 219.524974 \nL 181.64282 215.345846 \nL 182.278235 215.233071 \nL 182.913649 218.089467 \nL 183.549064 215.758515 \nL 184.184479 220.120364 \nL 184.819893 218.62979 \nL 185.455308 219.964602 \nL 186.726138 212.242331 \nL 187.361552 214.255488 \nL 187.996967 214.311479 \nL 188.632382 217.963277 \nL 189.267796 219.537138 \nL 189.903211 218.704 \nL 190.538626 216.098938 \nL 191.17404 219.777247 \nL 191.809455 219.150238 \nL 192.44487 218.901357 \nL 193.080284 220.570782 \nL 193.715699 217.578851 \nL 194.351114 218.595361 \nL 194.986529 221.32525 \nL 195.621943 219.199278 \nL 196.257358 220.1854 \nL 196.892773 218.341717 \nL 197.528187 222.30498 \nL 198.163602 218.74046 \nL 198.799017 216.77996 \nL 199.434431 221.595889 \nL 200.069846 217.625879 \nL 200.705261 220.793342 \nL 201.340675 213.409324 \nL 201.97609 213.565813 \nL 202.611505 216.627239 \nL 203.246919 218.040841 \nL 203.882334 220.797377 \nL 204.517749 217.656718 \nL 205.153164 219.655231 \nL 205.788578 214.263349 \nL 206.423993 212.963639 \nL 207.059408 218.058114 \nL 207.694822 213.390754 \nL 208.330237 220.967103 \nL 208.965652 220.847189 \nL 209.601066 219.457084 \nL 210.236481 217.254293 \nL 210.871896 213.288561 \nL 211.50731 220.102447 \nL 212.142725 217.827529 \nL 212.77814 216.61803 \nL 213.413555 219.111958 \nL 214.684384 221.705184 \nL 215.319799 218.301459 \nL 215.955213 215.859666 \nL 216.590628 219.453941 \nL 217.226043 220.511401 \nL 217.861457 213.619837 \nL 218.496872 212.108497 \nL 219.132287 217.874755 \nL 219.767701 218.6989 \nL 220.403116 223.296504 \nL 221.038531 216.320697 \nL 221.673945 215.942989 \nL 222.30936 216.280137 \nL 222.944775 218.856212 \nL 223.58019 214.810893 \nL 224.215604 217.944032 \nL 224.851019 215.979779 \nL 225.486434 212.268427 \nL 226.121848 215.360852 \nL 226.757263 215.491648 \nL 227.392678 221.936072 \nL 228.028092 216.302775 \nL 228.663507 221.013174 \nL 229.298922 218.162361 \nL 229.934336 217.317554 \nL 230.569751 213.302271 \nL 231.205166 218.281744 \nL 231.840581 214.252807 \nL 232.475995 219.266486 \nL 233.11141 215.901131 \nL 233.746825 221.597884 \nL 234.382239 215.71368 \nL 235.017654 221.288649 \nL 235.653069 218.746229 \nL 236.288483 218.448896 \nL 236.923898 213.820324 \nL 237.559313 219.271169 \nL 238.194727 220.307949 \nL 238.830142 213.716932 \nL 239.465557 214.271132 \nL 240.100971 218.942115 \nL 240.736386 212.957585 \nL 241.371801 218.746512 \nL 242.007216 216.527593 \nL 242.64263 220.717222 \nL 243.278045 219.894125 \nL 243.91346 221.463175 \nL 244.548874 219.938038 \nL 245.184289 217.957766 \nL 245.819704 219.813743 \nL 246.455118 214.490652 \nL 247.090533 223.215241 \nL 247.725948 210.403986 \nL 248.361362 221.147658 \nL 248.996777 214.460759 \nL 249.632192 218.217449 \nL 250.267607 220.885049 \nL 250.903021 218.518212 \nL 251.538436 222.950937 \nL 252.173851 219.110987 \nL 252.809265 213.251149 \nL 253.44468 217.515901 \nL 254.080095 218.179157 \nL 254.715509 213.560777 \nL 255.350924 217.819213 \nL 255.986339 219.680905 \nL 256.621753 215.809625 \nL 257.257168 216.224233 \nL 257.892583 220.193149 \nL 258.527997 218.911934 \nL 259.163412 216.36721 \nL 259.798827 222.519159 \nL 260.434242 221.821707 \nL 261.069656 217.823277 \nL 261.705071 217.962662 \nL 262.340486 222.727995 \nL 262.9759 213.759655 \nL 263.611315 221.080015 \nL 264.24673 220.311726 \nL 264.882144 220.033773 \nL 265.517559 220.502738 \nL 266.152974 219.869254 \nL 266.788388 218.04512 \nL 267.423803 221.917328 \nL 268.059218 214.066219 \nL 268.694633 218.152428 \nL 269.330047 219.608076 \nL 269.965462 219.127443 \nL 270.600877 221.889847 \nL 271.236291 221.915726 \nL 271.871706 224.291553 \nL 272.507121 216.31516 \nL 273.77795 220.115241 \nL 274.413365 220.177326 \nL 275.048779 218.115865 \nL 275.684194 218.785359 \nL 276.955023 224.439724 \nL 277.590438 218.226891 \nL 278.225853 222.318714 \nL 278.861268 217.223301 \nL 279.496682 217.75061 \nL 280.132097 220.884022 \nL 280.767512 220.760137 \nL 281.402926 221.829874 \nL 282.038341 214.179731 \nL 282.673756 215.560724 \nL 283.30917 211.025529 \nL 283.944585 216.054495 \nL 284.58 216.5115 \nL 285.215414 220.400557 \nL 285.850829 217.413846 \nL 286.486244 216.119147 \nL 287.121659 217.54364 \nL 287.757073 220.341572 \nL 288.392488 217.217753 \nL 289.027903 218.026493 \nL 289.663317 216.803624 \nL 290.298732 218.860036 \nL 290.934147 218.069662 \nL 291.569561 219.933773 \nL 292.204976 219.214931 \nL 292.840391 220.502506 \nL 293.475805 220.125015 \nL 294.11122 210.384338 \nL 294.746635 221.840373 \nL 295.382049 217.685652 \nL 296.017464 220.005122 \nL 296.652879 218.302591 \nL 297.288294 217.269116 \nL 297.923708 212.91691 \nL 298.559123 218.225417 \nL 299.194538 218.110275 \nL 299.829952 222.65663 \nL 300.465367 222.614804 \nL 301.100782 220.137111 \nL 301.736196 219.38848 \nL 302.371611 221.103108 \nL 303.007026 219.419708 \nL 303.64244 221.0078 \nL 304.277855 219.322615 \nL 304.91327 221.744469 \nL 305.548685 218.096967 \nL 306.184099 220.498537 \nL 306.819514 215.384533 \nL 307.454929 215.043372 \nL 308.090343 217.033423 \nL 308.725758 225.908101 \nL 309.361173 222.24979 \nL 309.996587 223.276105 \nL 310.632002 220.430006 \nL 311.267417 221.980862 \nL 311.902831 212.084522 \nL 312.538246 220.827368 \nL 313.809075 217.119665 \nL 314.44449 219.219252 \nL 315.079905 216.843946 \nL 315.71532 220.356796 \nL 316.350734 217.557501 \nL 316.986149 215.512334 \nL 318.256978 221.018352 \nL 318.892393 219.242029 \nL 319.527808 220.610127 \nL 320.163222 217.771757 \nL 320.798637 220.265217 \nL 321.434052 220.776614 \nL 322.069466 218.284393 \nL 322.704881 218.238269 \nL 323.340296 221.09036 \nL 323.975711 219.926135 \nL 324.611125 217.087772 \nL 325.24654 220.655549 \nL 325.881955 215.847696 \nL 326.517369 213.714199 \nL 327.152784 215.5413 \nL 327.788199 220.141742 \nL 328.423613 219.887485 \nL 329.059028 212.406438 \nL 329.694443 222.543859 \nL 330.329857 216.543771 \nL 330.965272 219.839856 \nL 331.600687 218.164355 \nL 332.236101 220.906295 \nL 332.871516 220.518863 \nL 333.506931 220.772626 \nL 334.142346 221.405334 \nL 334.77776 218.93871 \nL 335.413175 219.757876 \nL 336.04859 220.284723 \nL 336.684004 219.523262 \nL 337.319419 214.078992 \nL 337.954834 211.433014 \nL 338.590248 219.07124 \nL 339.225663 218.986771 \nL 339.861078 219.643018 \nL 340.496492 221.249397 \nL 341.131907 221.596598 \nL 341.767322 220.682608 \nL 342.402737 214.030706 \nL 343.038151 219.48022 \nL 343.673566 220.69757 \nL 344.944395 212.94894 \nL 345.57981 216.813756 \nL 346.215225 221.608713 \nL 346.850639 223.027587 \nL 347.486054 219.978547 \nL 348.121469 212.740472 \nL 348.756883 222.348729 \nL 349.392298 215.71645 \nL 350.027713 219.804851 \nL 350.663127 218.364115 \nL 351.298542 220.604655 \nL 351.933957 216.00459 \nL 352.569372 217.178046 \nL 353.204786 220.474403 \nL 353.840201 215.424469 \nL 354.475616 219.788784 \nL 355.11103 220.49778 \nL 355.746445 219.154139 \nL 356.38186 216.266746 \nL 357.017274 219.373532 \nL 357.652689 221.578948 \nL 358.288104 222.796298 \nL 358.923518 219.947682 \nL 359.558933 216.067737 \nL 360.194348 218.809442 \nL 360.829763 218.472814 \nL 361.465177 216.419639 \nL 362.100592 222.425892 \nL 362.736007 218.451877 \nL 363.371421 212.89126 \nL 364.642251 221.701606 \nL 365.277665 219.31754 \nL 366.548495 216.184308 \nL 367.183909 216.661337 \nL 367.819324 216.046497 \nL 368.454739 220.043302 \nL 369.090153 221.127799 \nL 369.725568 225.903497 \nL 369.725568 225.903497 \n\" style=\"fill:none;stroke:#4169e1;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 228.439219 \nL 50.14375 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 228.439219 \nL 384.94375 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 228.439219 \nL 384.94375 228.439219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 10.999219 \nL 384.94375 10.999219 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 317.060938 48.355469 \nL 377.94375 48.355469 \nQ 379.94375 48.355469 379.94375 46.355469 \nL 379.94375 17.999219 \nQ 379.94375 15.999219 377.94375 15.999219 \nL 317.060938 15.999219 \nQ 315.060938 15.999219 315.060938 17.999219 \nL 315.060938 46.355469 \nQ 315.060938 48.355469 317.060938 48.355469 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_15\">\n     <path d=\"M 319.060938 24.097656 \nL 339.060938 24.097656 \n\" style=\"fill:none;stroke:#8b0000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_16\"/>\n    <g id=\"text_15\">\n     <!-- SGD -->\n     <g transform=\"translate(347.060938 27.597656)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 59.515625 10.40625 \nL 59.515625 29.984375 \nL 43.40625 29.984375 \nL 43.40625 38.09375 \nL 69.28125 38.09375 \nL 69.28125 6.78125 \nQ 63.578125 2.734375 56.6875 0.65625 \nQ 49.8125 -1.421875 42 -1.421875 \nQ 24.90625 -1.421875 15.25 8.5625 \nQ 5.609375 18.5625 5.609375 36.375 \nQ 5.609375 54.25 15.25 64.234375 \nQ 24.90625 74.21875 42 74.21875 \nQ 49.125 74.21875 55.546875 72.453125 \nQ 61.96875 70.703125 67.390625 67.28125 \nL 67.390625 56.78125 \nQ 61.921875 61.421875 55.765625 63.765625 \nQ 49.609375 66.109375 42.828125 66.109375 \nQ 29.4375 66.109375 22.71875 58.640625 \nQ 16.015625 51.171875 16.015625 36.375 \nQ 16.015625 21.625 22.71875 14.15625 \nQ 29.4375 6.6875 42.828125 6.6875 \nQ 48.046875 6.6875 52.140625 7.59375 \nQ 56.25 8.5 59.515625 10.40625 \nz\n\" id=\"DejaVuSans-71\"/>\n       <path d=\"M 19.671875 64.796875 \nL 19.671875 8.109375 \nL 31.59375 8.109375 \nQ 46.6875 8.109375 53.6875 14.9375 \nQ 60.6875 21.78125 60.6875 36.53125 \nQ 60.6875 51.171875 53.6875 57.984375 \nQ 46.6875 64.796875 31.59375 64.796875 \nz\nM 9.8125 72.90625 \nL 30.078125 72.90625 \nQ 51.265625 72.90625 61.171875 64.09375 \nQ 71.09375 55.28125 71.09375 36.53125 \nQ 71.09375 17.671875 61.125 8.828125 \nQ 51.171875 0 30.078125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-68\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-83\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-71\"/>\n      <use x=\"140.966797\" xlink:href=\"#DejaVuSans-68\"/>\n     </g>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 319.060938 38.775781 \nL 339.060938 38.775781 \n\" style=\"fill:none;stroke:#4169e1;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"text_16\">\n     <!-- Adam -->\n     <g transform=\"translate(347.060938 42.275781)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n       <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"66.658203\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"130.134766\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"191.414062\" xlink:href=\"#DejaVuSans-109\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd98a694769\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"10.999219\"/>\n  </clipPath>\n </defs>\n</svg>\n","image/png":"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"},"metadata":{"needs_background":"light"}}],"metadata":{"id":"0ef0bbe5"}},{"cell_type":"markdown","source":["- 그래프를 통해 Adam 의 Loss 값이 SGD에서 더 낮은것을 확인할 수 있습니다! 실제로 베이스라인 코드에 적용해 볼까요?"],"metadata":{"id":"bb6e694b"}},{"cell_type":"markdown","source":["### 2.2 베이스라인 코드에 적용하며 이해하기\n","\n","- 이번엔 베이스라인 코드에서 SGD Optimizer를 먼저 적용해 보고 Adam Optimizer를 적용한 결과의 Loss와 Accuracy의 변화를 비교해 살펴보겠습니다.\n","- 결론 부터 말씀드리면 Adam(Adaptive moment estimation) Optimizer 는 다른 stochastic optimization methods(SGD, Adagrad, RMSProp)에 비해 optimizer 성능이 좋은 편입니다! 실제로 확인해볼까요? [참고](https://arxiv.org/pdf/1412.6980.pdf)\n"],"metadata":{"id":"3f70ca3a"}},{"cell_type":"markdown","source":["- 다음은 베이스라인 코드중 train.py 파일의 일부분 입니다. \n","\n","###### Libraries & Configurations"],"metadata":{"id":"2b5571a7"}},{"cell_type":"code","execution_count":7,"source":["import argparse\n","import glob\n","import json\n","import os\n","import random\n","import re\n","from importlib import import_module\n","from pathlib import Path\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torch.optim.lr_scheduler import StepLR\n","from torch.utils.data import Subset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from dataset import MaskBaseDataset\n","from loss import create_criterion\n","\n","\n","def seed_everything(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']"],"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'dataset'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-c828d2cb2cdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaskBaseDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"]}],"metadata":{"id":"ec5887c5"}},{"cell_type":"markdown","source":["###### Loss"],"metadata":{"id":"340d2986"}},{"cell_type":"code","execution_count":null,"source":["# -- loss & metric\n","criterion = create_criterion(args.criterion)  # default: cross_entropy"],"outputs":[],"metadata":{"id":"a4878c25"}},{"cell_type":"markdown","source":["- 아래쪽의 Optimizer를 적용하는 부분은 다음과 같이 간단히 한줄로 적용할 수도 있습니다.\n","###### SGD optimizer\n","    optimizer = SGD(model.parameters(), lr=lr, weight_decay=5e-4)\n","###### Adam optimizer\n","    optimizer = Adam(model.parameters(), lr=lr, weight_decay=5e-4)"],"metadata":{"id":"05c831b4"}},{"cell_type":"markdown","source":["###### Optimizer"],"metadata":{"id":"e008b57e"}},{"cell_type":"code","execution_count":null,"source":["opt_module = getattr(import_module(\"torch.optim\"), args.optimizer)  # default: SGD\n","optimizer = opt_module(\n","    filter(lambda p: p.requires_grad, model.parameters()),\n","    lr=args.lr,\n","    weight_decay=5e-4\n",")"],"outputs":[],"metadata":{"id":"9d21259b"}},{"cell_type":"markdown","source":["###### Scheduler"],"metadata":{"id":"641da963"}},{"cell_type":"code","execution_count":null,"source":["scheduler = StepLR(optimizer, args.lr_decay_step, gamma=0.5)"],"outputs":[],"metadata":{"id":"bcf2e49c"}},{"cell_type":"markdown","source":["- SGD Optimizer 결과."],"metadata":{"id":"5f642b9d"}},{"cell_type":"markdown","source":["Epoch[98/100](20/241) || training loss 2.278 || training accuracy 26.56% || lr 6.25e-05\n","\n","Epoch[98/100](40/241) || training loss 2.261 || training accuracy 28.44% || lr 6.25e-05\n","\n","Epoch[98/100](60/241) || training loss 2.258 || training accuracy 28.75% || lr 6.25e-05\n","\n","Epoch[98/100](80/241) || training loss 2.286 || training accuracy 28.83% || lr 6.25e-05\n","\n","Epoch[98/100](100/241) || training loss  2.3 || training accuracy 28.05% || lr 6.25e-05\n","\n","Epoch[98/100](120/241) || training loss 2.298 || training accuracy 26.48% || lr 6.25e-05\n","\n","Epoch[98/100](140/241) || training loss 2.232 || training accuracy 28.75% || lr 6.25e-05\n","\n","Epoch[98/100](160/241) || training loss 2.338 || training accuracy 26.56% || lr 6.25e-05\n","\n","Epoch[98/100](180/241) || training loss 2.206 || training accuracy 29.61% || lr 6.25e-05\n","\n","Epoch[98/100](200/241) || training loss  2.3 || training accuracy 25.86% || lr 6.25e-05\n","\n","Epoch[98/100](220/241) || training loss 2.269 || training accuracy 27.89% || lr 6.25e-05\n","\n","Epoch[98/100](240/241) || training loss 2.294 || training accuracy 27.97% || lr 6.25e-05\n","\n","Calculating validation results...\n","[Val] acc : 24.71%, loss:  2.3 || best acc : 24.80%, best loss:  2.3\n","\n","Epoch[99/100](20/241) || training loss 2.258 || training accuracy 28.98% || lr 6.25e-05\n","\n","Epoch[99/100](40/241) || training loss 2.273 || training accuracy 27.66% || lr 6.25e-05\n","\n","Epoch[99/100](60/241) || training loss 2.299 || training accuracy 25.47% || lr 6.25e-05\n","\n","Epoch[99/100](80/241) || training loss 2.325 || training accuracy 26.02% || lr 6.25e-05\n","\n","Epoch[99/100](100/241) || training loss 2.27 || training accuracy 26.25% || lr 6.25e-05\n","\n","Epoch[99/100](120/241) || training loss 2.237 || training accuracy 29.38% || lr 6.25e-05\n","\n","Epoch[99/100](140/241) || training loss 2.281 || training accuracy 27.89% || lr 6.25e-05\n","\n","Epoch[99/100](160/241) || training loss 2.264 || training accuracy 28.91% || lr 6.25e-05\n","\n","Epoch[99/100](180/241) || training loss 2.256 || training accuracy 30.16% || lr 6.25e-05\n","\n","Epoch[99/100](200/241) || training loss 2.304 || training accuracy 28.12% || lr 6.25e-05\n","\n","Epoch[99/100](220/241) || training loss 2.262 || training accuracy 29.38% || lr 6.25e-05\n","\n","Epoch[99/100](240/241) || training loss 2.282 || training accuracy 27.42% || lr 6.25e-05\n","\n","Calculating validation results...\n","[Val] acc : 24.80%, loss:  2.3 || best acc : 24.80%, best loss:  2.3"],"metadata":{"id":"66195987"}},{"cell_type":"markdown","source":["- Adam Optimizer 결과로, Optimizer 만 바꿨을 뿐인데 확연히 낮아진 Loss값과 향상된 Accuracy 를 확인할 수 있습니다"],"metadata":{"id":"b9df4610"}},{"cell_type":"markdown","source":["Epoch[98/100](20/241) || training loss 0.3912 || training accuracy 86.72% || lr 6.25e-05\n","\n","Epoch[98/100](40/241) || training loss 0.4169 || training accuracy 86.72% || lr 6.25e-05\n","\n","Epoch[98/100](60/241) || training loss 0.4019 || training accuracy 86.56% || lr 6.25e-05\n","\n","Epoch[98/100](80/241) || training loss 0.428 || training accuracy 85.70% || lr 6.25e-05\n","\n","Epoch[98/100](100/241) || training loss 0.4092 || training accuracy 85.78% || lr 6.25e-05\n","\n","Epoch[98/100](120/241) || training loss 0.4237 || training accuracy 86.33% || lr 6.25e-05\n","\n","Epoch[98/100](140/241) || training loss 0.4093 || training accuracy 86.25% || lr 6.25e-05\n","\n","Epoch[98/100](160/241) || training loss 0.3903 || training accuracy 87.19% || lr 6.25e-05\n","\n","Epoch[98/100](180/241) || training loss 0.4056 || training accuracy 86.64% || lr 6.25e-05\n","\n","Epoch[98/100](200/241) || training loss 0.408 || training accuracy 86.72% || lr 6.25e-05\n","\n","Epoch[98/100](220/241) || training loss 0.4367 || training accuracy 84.92% || lr 6.25e-05\n","\n","Epoch[98/100](240/241) || training loss 0.4363 || training accuracy 85.55% || lr 6.25e-05\n","\n","Calculating validation results...\n","[Val] acc : 68.12%, loss: 0.71 || best acc : 68.30%, best loss:  0.7\n","\n","Epoch[99/100](20/241) || training loss 0.3987 || training accuracy 87.11% || lr 6.25e-05\n","\n","Epoch[99/100](40/241) || training loss 0.4065 || training accuracy 87.89% || lr 6.25e-05\n","\n","Epoch[99/100](60/241) || training loss 0.4084 || training accuracy 86.25% || lr 6.25e-05\n","\n","Epoch[99/100](80/241) || training loss 0.4135 || training accuracy 86.56% || lr 6.25e-05\n","\n","Epoch[99/100](100/241) || training loss 0.4168 || training accuracy 85.78% || lr 6.25e-05\n","\n","Epoch[99/100](120/241) || training loss 0.3895 || training accuracy 86.48% || lr 6.25e-05\n","\n","Epoch[99/100](140/241) || training loss 0.4228 || training accuracy 85.62% || lr 6.25e-05\n","\n","Epoch[99/100](160/241) || training loss 0.4314 || training accuracy 85.86% || lr 6.25e-05\n","\n","Epoch[99/100](180/241) || training loss 0.4434 || training accuracy 84.92% || lr 6.25e-05\n","\n","Epoch[99/100](200/241) || training loss 0.4697 || training accuracy 84.53% || lr 6.25e-05\n","\n","Epoch[99/100](220/241) || training loss 0.3847 || training accuracy 87.73% || lr 6.25e-05\n","\n","Epoch[99/100](240/241) || training loss 0.4097 || training accuracy 85.86% || lr 6.25e-05\n","\n","Calculating validation results...\n","[Val] acc : 67.80%, loss:  0.7 || best acc : 68.30%, best loss:  0.7"],"metadata":{"id":"ef82e7f0"}},{"cell_type":"markdown","source":["## 3. Scheduler\n","- Scheduler은 optimizer의 learning rate를 동적으로 변경시키는 기능을 합니다.\n","- Optimizer과 Scheduler를 적절히 활용하면 모델이 좋은 성능으로 Fitting하는데 도움을 줍니다."],"metadata":{"id":"informative-merchandise"}},{"cell_type":"code","execution_count":null,"source":["# -- scheduler: StepLR\n","# 지정된 step마다 learning rate를 감소시킵니다.\n","scheduler = StepLR(optimizer, lr_decay_step, gamma=0.5)"],"outputs":[],"metadata":{"id":"original-breath"}},{"cell_type":"code","execution_count":null,"source":["# -- scheduler: ReduceLROnPlateau\n","# 성능이 향상되지 않을 때 learning rate를 줄입니다. patience=10은 10회 동안 성능 향상이 없을 경우입니다.\n","scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=10)"],"outputs":[],"metadata":{"id":"resistant-compiler"}},{"cell_type":"code","execution_count":null,"source":["# -- scheduler: CosineAnnealingLR\n","# CosineAnnealing은 learning rate를 cosine 그래프처럼 변화시킵니다.\n","scheduler = CosineAnnealingLR(optimizer, T_max=2, eta_min=0.)"],"outputs":[],"metadata":{"id":"distant-crowd"}},{"cell_type":"markdown","source":["## 4. Metric\n","- Classification 성능을 표현할 때 다양한 평가지표가 있습니다.\n","- Accuracy: 모델이 정확하게 예측한 객체의 비율\n","- True Positive(TP): 실제 True인 정답을 True라고 예측 (정답)\n","- False Positive(FP): 실제 False인 정답을 True라고 예측 (오답)\n","- False Negative(FN): 실제 True인 정답을 False라고 예측 (오답)\n","- True Negative(TN): 실제 False인 정답을 False라고 예측 (정답)\n","- Precision(정밀도): TP / (TP + FP)\n","- Recall(재현율): TP / (TP + FN)"],"metadata":{"id":"under-documentary"}},{"cell_type":"code","execution_count":null,"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","y_true = [0, 1, 2, 0, 1, 2]\n","y_pred = [0, 2, 1, 0, 0, 1]"],"outputs":[],"metadata":{"id":"racial-zealand"}},{"cell_type":"code","execution_count":null,"source":["# -- Accuracy\n","accuracy_score(y_true, y_pred)"],"outputs":[],"metadata":{"id":"union-laundry"}},{"cell_type":"code","execution_count":null,"source":["# -- Accuracy\n","# Normalize를 안하면 맞춘 개수가 표시된다\n","accuracy_score(y_true, y_pred, normalize=False)"],"outputs":[],"metadata":{"id":"rotary-working"}},{"cell_type":"code","execution_count":null,"source":["# -- Precision\n","precision = precision_score(y_true, y_pred, average='macro')\n","precision"],"outputs":[],"metadata":{"id":"employed-torture"}},{"cell_type":"code","execution_count":null,"source":["# -- Recall\n","recall = recall_score(y_true, y_pred, average='macro')\n","recall"],"outputs":[],"metadata":{"id":"designed-storage"}},{"cell_type":"code","execution_count":null,"source":["# -- f1 score\n","2 * (precision * recall) / (precision + recall)"],"outputs":[],"metadata":{"id":"informed-consolidation"}},{"cell_type":"code","execution_count":null,"source":["# -- f1 score (sklearn)\n","f1_score(y_true, y_pred, average='macro')"],"outputs":[],"metadata":{"id":"owned-victory"}},{"cell_type":"markdown","source":["## 5. Training process"],"metadata":{"id":"foreign-guess"}},{"cell_type":"code","execution_count":null,"source":["dataset = MaskMultiClassDataset(img_root, label_path, 'train')\n","n_val = int(len(dataset) * val_split)\n","n_train = len(dataset) - n_val\n","train_set, val_set = torch.utils.data.random_split(dataset, [n_train, n_val])\n","val_set.dataset.set_phase(\"test\")  # todo : fix\n","\n","train_loader = torch.utils.data.DataLoader(\n","    train_set,\n","    batch_size=batch_size,\n","    num_workers=num_workers,\n","    drop_last=True,\n",")\n","\n","val_loader = torch.utils.data.DataLoader(\n","    val_set,\n","    batch_size=batch_size,\n","    num_workers=num_workers,\n","    drop_last=True,\n",")"],"outputs":[],"metadata":{"id":"several-principal"}},{"cell_type":"markdown","source":["### 5.1 Callback - Checkpoint, Early Stopping"],"metadata":{"id":"bulgarian-moisture"}},{"cell_type":"code","execution_count":null,"source":["# -- Callback1: Checkpoint - Accuracy가 높아질 때마다 모델을 저장합니다.\n","# 학습 코드에서 이어집니다.\n","\n","# -- Callback2: Early Stopping - 성능이 일정 기간동안 향상이 없을 경우 학습을 종료합니다.\n","patience = 10\n","counter = 0\n","# 학습 코드에서 이어집니다."],"outputs":[],"metadata":{"id":"insured-craft"}},{"cell_type":"markdown","source":["### 5.2 Training Method - Gradient Accumulation\n","- Graident Accumulation은 한 iteration에 파라미터를 업데이트시키는게 아니라, gradient를 여러 iteration 동안 쌓아서 업데이트시킵니다. 한 번에 파라미터를 업데이트시키는 건 noise가 있을 수 있으므로, 여러번 쌓아서 한번에 업데이트 시킴으로써 그러한 문제를 방지하기 위함입니다."],"metadata":{"id":"turned-citizenship"}},{"cell_type":"code","execution_count":null,"source":["# -- Gradient Accumulation\n","accumulation_steps = 2\n","# 학습코드에서 이어집니다."],"outputs":[],"metadata":{"id":"bibliographic-recycling"}},{"cell_type":"markdown","source":["### 5.3 Training Loop"],"metadata":{"id":"premium-poker"}},{"cell_type":"code","execution_count":null,"source":["os.makedirs(os.path.join(os.getcwd(), 'results', name), exist_ok=True)\n","\n","counter = 0\n","best_val_acc = 0\n","best_val_loss = np.inf\n","for epoch in range(num_epochs):\n","    # train loop\n","    model.train()\n","    loss_value = 0\n","    matches = 0\n","    for idx, train_batch in enumerate(train_loader):\n","        inputs, labels = train_batch\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        outs = model(inputs)\n","        preds = torch.argmax(outs, dim=-1)\n","        loss = criterion(outs, labels)\n","\n","        loss.backward()\n","        \n","        # -- Gradient Accumulation\n","        if (idx+1) % accumulation_steps == 0:\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","        loss_value += loss.item()\n","        matches += (preds == labels).sum().item()\n","        if (idx + 1) % train_log_interval == 0:\n","            train_loss = loss_value / train_log_interval\n","            train_acc = matches / batch_size / train_log_interval\n","            current_lr = scheduler.get_last_lr()\n","            print(\n","                f\"Epoch[{epoch}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \"\n","                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n","            )\n","\n","            loss_value = 0\n","            matches = 0\n","\n","    scheduler.step()\n","\n","    # val loop\n","    with torch.no_grad():\n","        print(\"Calculating validation results...\")\n","        model.eval()\n","        val_loss_items = []\n","        val_acc_items = []\n","        for val_batch in val_loader:\n","            inputs, labels = val_batch\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outs = model(inputs)\n","            preds = torch.argmax(outs, dim=-1)\n","\n","            loss_item = criterion(outs, labels).item()\n","            acc_item = (labels == preds).sum().item()\n","            val_loss_items.append(loss_item)\n","            val_acc_items.append(acc_item)\n","\n","        val_loss = np.sum(val_loss_items) / len(val_loader)\n","        val_acc = np.sum(val_acc_items) / len(val_set)\n","        \n","        # Callback1: validation accuracy가 향상될수록 모델을 저장합니다.\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","        if val_acc > best_val_acc:\n","            print(\"New best model for val accuracy! saving the model..\")\n","            torch.save(model.state_dict(), f\"results/{name}/{epoch:03}_accuracy_{val_acc:4.2%}.ckpt\")\n","            best_val_acc = val_acc\n","            counter = 0\n","        else:\n","            counter += 1\n","        # Callback2: patience 횟수 동안 성능 향상이 없을 경우 학습을 종료시킵니다.\n","        if counter > patience:\n","            print(\"Early Stopping...\")\n","            break\n","        \n","        \n","        print(\n","            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n","            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n","        )"],"outputs":[],"metadata":{"id":"24b612c2"}},{"cell_type":"markdown","source":["## 6. Reference\n","- [sumni blog post](https://sumniya.tistory.com/26)\n","- [How to create a custom loss function in PyTorch](https://neptune.ai/blog/pytorch-loss-functions)"],"metadata":{"id":"alert-marathon"}}]}