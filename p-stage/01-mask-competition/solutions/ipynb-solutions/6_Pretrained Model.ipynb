{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"5_Pretrained Model.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"-MS3bWUNUSgz"},"source":["## Lesson 6 - Pretrained Model\n"," - 이번 실습 자료에서는 강의시간에 다루었던 torchvision 을 사용하여 pretrained 모델을 사용하는 방법에 대해 실습하겠습니다.\n"," - torchvision 의 pretrained model 리스트는 다음과 같습니다\n"," \n"," [List of torchvision models](https://github.com/pytorch/vision/blob/master/torchvision/models/__init__.py#L1-L14)\n","```\n","from .alexnet import *\n","from .resnet import *\n","from .vgg import *\n","from .squeezenet import *\n","from .inception import *\n","from .densenet import *\n","from .googlenet import *\n","from .mobilenet import *\n","from .mnasnet import *\n","from .shufflenetv2 import *\n","from . import segmentation\n","from . import detection\n","from . import video\n","from . import quantization\n","```"],"id":"-MS3bWUNUSgz"},{"cell_type":"code","metadata":{"id":"QfXG-tzwUSgv"},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"id":"QfXG-tzwUSgv","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D2BlaYvVUSg0"},"source":["#### 가장 기본이라고 할 수 있는 Alextnet 모델 아키텍쳐를 사용해보겠습니다."],"id":"D2BlaYvVUSg0"},{"cell_type":"code","metadata":{"scrolled":true,"id":"8PT7gW-wUSg0"},"source":["from torchvision.models import alexnet\n","model = alexnet()\n","model"],"id":"8PT7gW-wUSg0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kzOWtmBxUSg2"},"source":["#### Alexnet 의 pretrained 버전 또한 쉽게 불러올 수 있습니다."],"id":"kzOWtmBxUSg2"},{"cell_type":"code","metadata":{"scrolled":true,"id":"36Rqc5-OUSg2"},"source":["model = alexnet(pretrained=True)\n","model"],"id":"36Rqc5-OUSg2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3799RyCBUSg3"},"source":["#### torchvision 에서 해당 모델을 어떤 식으로 구현하였는지 직접 확인해보면 매우 도움이 많으 됩니다.\n","Example:\n","[source code](https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py#L15-L50)\n","```\n","class AlexNet(nn.Module):\n","\n","    def __init__(self, num_classes=1000):\n","        super(AlexNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2),\n","        )\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(),\n","            nn.Linear(256 * 6 * 6, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(4096, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x\n","```"],"id":"3799RyCBUSg3"},{"cell_type":"markdown","metadata":{"id":"GCagXJS2USg4"},"source":["#### 다른 모델들( e.g. vgg19, resnet18) 도 같은 방법으로 손 쉽게 사용할 수 있습니다."],"id":"GCagXJS2USg4"},{"cell_type":"code","metadata":{"scrolled":true,"id":"tsZbkfObUSg5"},"source":["from torchvision.models import vgg19_bn\n","model = vgg19_bn(pretrained=True)\n","model"],"id":"tsZbkfObUSg5","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"foiF_enTUSg5"},"source":["#### Pretrained 모델을 내 태스크에 맞게 어떻게 사용할 수 있나요?\n"," - Trochvision 모델들은 보통 feature-extraction 파트, task-specific 파트로 크게 두 가지로 구성되어 있습니다.\n"," - Task specific 파트는 모델의 태스크(이미지 분류, 객체 인식 등) 에 따라 모두 다릅니다.\n"," - 심지어 같은 이미지 분류 안에서도, 어떤 데이터셋으로 pretrain 하였느냐에 따라 다를 수 있습니다.\n"," - 따라서, 우리도 우리 테스크에 맞게 task specific 파트는 새로 정의하여 사용하여야 합니다."],"id":"foiF_enTUSg5"},{"cell_type":"markdown","metadata":{"id":"nYv1DZ69USg6"},"source":[" - 주로 이미지넷 데이터셋을 사용하여 pretrain 을 하기에 output_dim=1000 인 경우가 많습니다.\n"," - 따라서 우리 태스크의 클래스 갯수(18)에 맞게 재정의하여 사용할 수 있습니다."],"id":"nYv1DZ69USg6"},{"cell_type":"code","metadata":{"scrolled":true,"id":"xQIxW_QRUSg6"},"source":["num_classes = 18\n","model = vgg19_bn(pretrained=True)\n","model.classifier = nn.Sequential(\n","    nn.Linear(512 * 7 * 7, 4096),\n","    nn.ReLU(True),\n","    nn.Dropout(),\n","    nn.Linear(4096, 4096),\n","    nn.ReLU(True),\n","    nn.Dropout(),\n","    nn.Linear(4096, num_classes),\n",")\n","\n","model"],"id":"xQIxW_QRUSg6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bkuxurF2USg7"},"source":["#### Weight Freeze\n"," - Weight freeze 란 해당 모듈의 graident 는 역전파 하지 않아 학습을 하지 않는다는 의미입니다.\n"," - 예를 들어, 우리가 하려는 태스크가 pretrain 한 태스크와 매우 유사하다면, feature 파트는 freeze 하여 학습하지 않고 새로 정의한 task specific 파트만 학습하는 것이 좋은 방법일 수 있습니다.\n"," - weight freeze 는 `requires_grad` 를 사용하여 쉽게 구현할 수 있습니다. "],"id":"bkuxurF2USg7"},{"cell_type":"code","metadata":{"scrolled":true,"id":"Qr2vjssZUSg7"},"source":["# feature 파트만 freeze\n","model.features.requires_grad_(False)\n","for param, weight in model.named_parameters():\n","    print(f\"파라미터 {param:20} 가 gradient 를 tracking 하나요? -> {weight.requires_grad}\")"],"id":"Qr2vjssZUSg7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LM4aPLqeUSg8"},"source":["#### Weight initialization \n"," - weight 초기화는 종종 모델의 성능에 critical 한 영향을 줍니다.\n"," - 하지만 만약 pretrained 모델을 사용한다면 pretrained 부분은 초기화를 하지 말고, 재정의한 task specific 파트만 초기화하여야 합니다."],"id":"LM4aPLqeUSg8"},{"cell_type":"code","metadata":{"id":"I_5s0otKUSg8"},"source":["import torch.nn.init as init\n","\n","def initialize_weights(model):\n","    \"\"\"\n","    Xavier uniform 분포로 모든 weight 를 초기화합니다.\n","    더 많은 weight 초기화 방법은 다음 문서에서 참고해주세요. https://pytorch.org/docs/stable/nn.init.html\n","    \"\"\"\n","    for m in model.modules():\n","        if isinstance(m, nn.Conv2d):\n","            init.xavier_uniform_(m.weight.data)\n","            if m.bias is not None:\n","                m.bias.data.zero_()\n","        elif isinstance(m, nn.BatchNorm2d):\n","            m.weight.data.fill_(1)\n","            m.bias.data.zero_()\n","        elif isinstance(m, nn.Linear):\n","            m.weight.data.normal_(0, 0.01)\n","            m.bias.data.zero_()"],"id":"I_5s0otKUSg8","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mRDhT8vnUSg9"},"source":["#### pretrained 모델을 가져와 가장 앞단 layer 의 weight 분포를 봐봅시다"],"id":"mRDhT8vnUSg9"},{"cell_type":"code","metadata":{"id":"GNghuOsYUSg9"},"source":["import matplotlib.pyplot as plt\n","\n","model = vgg19_bn(pretrained=True)\n","\n","# Weight Initialization 이전 모델 feature 파트의 첫번째 weight 분포\n","plt.hist(model.features[0].weight.detach().numpy().reshape(-1))  \n","plt.show()"],"id":"GNghuOsYUSg9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yeAhmcpbUSg-"},"source":["#### weight 초기화 후 분포를 봐 봅시다\n"," - `xavier_uniform` 으로 초기화하여 웨이트들이 uniform 한 분포를 가지게 되었습니다."],"id":"yeAhmcpbUSg-"},{"cell_type":"code","metadata":{"id":"ISYQG_TDUSg-"},"source":["model = vgg19_bn(pretrained=True)\n","\n","# 모든 weight 를 initialize\n","initialize_weights(model.features)\n","\n","# Weight Initialization 이후 모델 feature 파트의 첫번째 weight 분포\n","# (xavier) uniform 한 분포로 바뀐 것을 확인할 수 있습니다.\n","plt.hist(model.features[0].weight.detach().numpy().reshape(-1))\n","plt.show()"],"id":"ISYQG_TDUSg-","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fbqr7onnUSg_"},"source":["#### task specific 한 부분만 초기화하엿습니다\n"," - feature extraction 파트는 초기화가 되지 않은 것은 확인할 수 있습니다."],"id":"fbqr7onnUSg_"},{"cell_type":"code","metadata":{"id":"kvlDANr6USg_"},"source":["model = vgg19_bn(pretrained=True)\n","\n","# Classifier 부분만 initialize\n","initialize_weights(model.classifier)\n","\n","# Weight Initialization 이후 모델 feature 파트의 첫번째 weight 분포\n","# classifier 부분만 xavier uniform 으로 초기화해서 feature 파트는 uniform 한 분포를 가지지 않는 것을 확인할 수 있습니다.\n","plt.hist(model.features[0].weight.detach().numpy().reshape(-1)) \n","plt.show()"],"id":"kvlDANr6USg_","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5rYLwDKUSg_"},"source":["## Appendix (optional)"],"id":"o5rYLwDKUSg_"},{"cell_type":"markdown","metadata":{"id":"DYtGOREKUShA"},"source":["### SOTA (State Of The Art)  모델을 리서치 하는 방법\n","- timm\n","- paper with code"],"id":"DYtGOREKUShA"},{"cell_type":"markdown","metadata":{"id":"2IlmlHvDUShA"},"source":["## timm (pyTorch IMage Models)\n","\n","PyTorch Image Models (timm) is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders / augmentations, and reference training / validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.\n","\n","#### References\n","https://github.com/rwightman/pytorch-image-models#introduction\n","\n","https://fastai.github.io/timmdocs/\n","\n","https://rwightman.github.io/pytorch-image-models/"],"id":"2IlmlHvDUShA"},{"cell_type":"code","metadata":{"id":"qLTgGr7EUShA"},"source":["!pip install timm"],"id":"qLTgGr7EUShA","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yF6B71PxUShA"},"source":["#### Timm 을 사용하여 pretrained 모델 불러오기"],"id":"yF6B71PxUShA"},{"cell_type":"code","metadata":{"id":"R2dV1z-1UShB"},"source":["import timm\n","\n","m = timm.create_model('mobilenetv3_large_100', pretrained=True)\n","m.eval()"],"id":"R2dV1z-1UShB","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NxpGBf1HUShB"},"source":["#### Timm 에서 사용가능한 pretrained 모델 목록"],"id":"NxpGBf1HUShB"},{"cell_type":"code","metadata":{"id":"MnZYn5SlUShB"},"source":["import timm\n","from pprint import pprint\n","model_names = timm.list_models(pretrained=True)\n","pprint(model_names)"],"id":"MnZYn5SlUShB","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GuK012_QUShC"},"source":["#### 다음과 같은 방법을 통해서 원하는 모델을 찾는 것도 가능합니다"],"id":"GuK012_QUShC"},{"cell_type":"code","metadata":{"id":"8q11NdteUShC"},"source":["import timm\n","from pprint import pprint\n","model_names = timm.list_models('*resne*t*')\n","pprint(model_names)"],"id":"8q11NdteUShC","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_3W3me_bUShC"},"source":["## Paper with code\n"," - https://paperswithcode.com/task/image-classification\n"," - 다양한 태스크와 데이터셋에 대한 다양한 모델들의 성능을 벤치마킹해주는 웹서비스입니다.\n"," - 해당 서비스를 통해 각 모델들의 성능 비교뿐 아니라 논문과 구현 코드로 forwarding 도 가능합니다."],"id":"_3W3me_bUShC"}]}